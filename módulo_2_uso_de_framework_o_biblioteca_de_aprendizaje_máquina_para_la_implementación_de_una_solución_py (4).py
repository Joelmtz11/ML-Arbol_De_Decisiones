# -*- coding: utf-8 -*-
"""Módulo 2 Uso de framework o biblioteca de aprendizaje máquina para la implementación de una solución.py

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1gYST3xM7dLtVhrMyMYrLofEehgXXDMyf

#Importación de bibliotecas:

Importas las bibliotecas necesarias, incluyendo pandas para la manipulación de datos, DecisionTreeClassifier de scikit-learn para el modelo de árbol de decisión, train_test_split para dividir los datos en conjuntos de entrenamiento y prueba, y métricas de scikit-learn para evaluar el rendimiento del modelo.
"""

# Commented out IPython magic to ensure Python compatibility.
# Load libraries
import pandas as pd
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn import metrics
import numpy as np
import matplotlib as mpl
import matplotlib.pyplot as plt
import seaborn as sn
# %matplotlib inline
from sklearn.preprocessing import StandardScaler
from scipy.stats import variation
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.model_selection import train_test_split
from sklearn.model_selection import RepeatedStratifiedKFold
from sklearn.model_selection import cross_val_score
import plotly.express as px
from sklearn.tree import export_graphviz
from sklearn import linear_model
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score, r2_score
from sklearn.metrics import classification_report, ConfusionMatrixDisplay
from sklearn import tree

"""Definición de nombres de columnas:

Defines los nombres de las columnas del conjunto de datos en col_names.

Carga de datos:

Cargas el conjunto de datos de diabetes desde un archivo CSV llamado "diabetes.csv" y lo almacenas en un DataFrame llamado pima. Los nombres de las columnas se asignan según col_names.
"""

col_names = ['pregnant', 'glucose', 'bp', 'skin', 'insulin', 'bmi', 'pedigree', 'age', 'diagnostic']
# load dataset
pima = pd.read_csv("diabetes.csv", header=None, names=col_names)

pima.head()

"""#Preprocesamiento de datos:

Eliminas la primera fila de pima con pima = pima.iloc[1:, :] (esto sirve para eliminar una fila de encabezado adicional que puede haber quedado después de cargar el archivo CSV).
"""

pima=pima.iloc[1:,:]

#split dataset in features and target variable
feature_cols = ['pregnant', 'insulin', 'bmi', 'age','glucose','bp','pedigree']
X = pima[feature_cols] # Features
y = pima.diagnostic # Target variable

pima.shape

pima.isna().count()

pima.dtypes

pima.describe()

"""#Análisis exploratorio con gráficas de barras"""

fig = px.bar(pima, x='age',color='diagnostic', color_discrete_map={
        'Diabetes' : 'turquoise',
        'No Diabetes': 'navy'})
fig.show()

fig = px.bar(pima, x='pregnant',color='diagnostic', color_discrete_map={
        'Diabetes' : 'turquoise',
        'No Diabetes': 'navy'})
fig.show()

fig = px.bar(pima, x='age',color='glucose', color_discrete_map={
        'Diabetes' : 'turquoise',
        'No Diabetes': 'navy'})
fig.show()

fig = px.bar(pima, x='age',color='insulin', color_discrete_map={
        'insuline ok' : 'turquoise',
        'No Diabetes': 'navy'})
fig.show()

"""#División de datos

Defines las características (X) y la variable objetivo (y) y luego divides los datos en conjuntos de entrenamiento (X_train, y_train) y prueba (X_test, y_test) utilizando train_test_split. El 70% de los datos se utiliza para entrenar el modelo, mientras que el 30% se utiliza para evaluarlo.
"""

# Split dataset into training set and test set
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1) # 70% training and 30% test

import matplotlib.pyplot as plt

# Contar el número de muestras en cada conjunto
train_count = len(y_train)
test_count = len(y_test)

# Etiquetar los conjuntos
labels = ['Conjunto de Entrenamiento', 'Conjunto de Prueba']
sizes = [train_count, test_count]
colors = ['lightblue', 'lightcoral']
explode = (0.1, 0)  # Explotar la primera rebanada (conjunto de entrenamiento)

# Crear un gráfico de pastel
plt.pie(sizes, explode=explode, labels=labels, colors=colors, autopct='%1.1f%%', shadow=True, startangle=140)
plt.axis('equal')  # Hace que el gráfico de pastel sea circular
plt.title('División de Datos de Entrenamiento y Prueba')
plt.show()

"""#Creación y entrenamiento del primer árbol de decisión:

Creas un objeto DecisionTreeClassifier llamado clf.
Entrenas el modelo de árbol de decisión en el conjunto de entrenamiento (X_train, y_train) utilizando fit.
"""

# Create Decision Tree classifer object
clf = DecisionTreeClassifier()

# Train Decision Tree Classifer
clf = clf.fit(X_train,y_train)

#Predict the response for test dataset
y_pred = clf.predict(X_test)

"""Matriz de confusión: La matriz de confusión  proporciona una visión detallada de cómo se están clasificando las muestras en diferentes categorías. Puedes calcular el número de falsos positivos, falsos negativos, verdaderos positivos y verdaderos negativos. Una alta proporción de falsos positivos o falsos negativos puede indicar sesgo en una dirección particular."""

from sklearn.metrics import confusion_matrix
confusion = confusion_matrix(y_test, y_pred)
print("Confusion Matrix:\n", confusion)

"""Precisión, exhaustividad y F1-score: Estas métricas proporcionan una medida más completa del rendimiento del modelo. Puedes calcular la precisión, la exhaustividad y el F1-score para cada clase y observar si hay un desequilibrio significativo en estas métricas."""

from sklearn.metrics import classification_report
report = classification_report(y_test, y_pred)
print("Classification Report:\n", report)

import matplotlib.pyplot as plt
from sklearn.metrics import classification_report
import numpy as np


# Generar el informe de clasificación
report = classification_report(y_test, y_pred, target_names=['Clase 0', 'Clase 1'], output_dict=True)

# Extraer métricas de precisión, recall y F1-score
precision = [report['Clase 0']['precision'], report['Clase 1']['precision']]
recall = [report['Clase 0']['recall'], report['Clase 1']['recall']]
f1_score = [report['Clase 0']['f1-score'], report['Clase 1']['f1-score']]

# Etiquetas de las métricas
labels = ['Clase 0', 'Clase 1']

# Coordenadas para las barras
x = np.arange(len(labels))
width = 0.2

# Crear la figura y los ejes
fig, ax = plt.subplots()

# Dibujar las barras
ax.bar(x - width, precision, width, label='Precisión', alpha=0.7)
ax.bar(x, recall, width, label='Recall', alpha=0.7)
ax.bar(x + width, f1_score, width, label='F1-Score', alpha=0.7)

# Configurar las etiquetas del eje x
ax.set_xlabel('Clases')
ax.set_xticks(x)
ax.set_xticklabels(labels)

# Configurar el título y la leyenda
ax.set_title('Métricas de Clasificación por Clase')
ax.legend()

# Mostrar la gráfica
plt.show()

"""Curva ROC y AUC: La curva ROC (Receiver Operating Characteristic) y el área bajo la curva (AUC) pueden ayudar a evaluar el rendimiento del modelo en problemas de clasificación binaria. Un sesgo en una dirección particular puede afectar la posición de la curva ROC."""

from sklearn.metrics import roc_curve, auc

# Convierte las etiquetas '0' y '1' a valores enteros 0 y 1
y_test_int = [int(label) for label in y_test]
y_pred_int = [int(label) for label in y_pred]

# Calcula la curva ROC y el AUC
fpr, tpr, _ = roc_curve(y_test_int, y_pred_int, pos_label=1)
roc_auc = auc(fpr, tpr)
print("AUC:", roc_auc)

plt.figure()
plt.plot(fpr, tpr, color='darkorange', lw=2, label='Curva ROC (area = %0.2f)' % roc_auc)
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('Tasa de Falsos Positivos')
plt.ylabel('Tasa de Verdaderos Positivos')
plt.title('Curva ROC')
plt.legend(loc="lower right")
plt.show()

"""Validación cruzada: Utiliza técnicas de validación cruzada para evaluar el rendimiento del modelo en múltiples divisiones del conjunto de datos. Esto te dará una idea más robusta del sesgo del modelo."""

from sklearn.model_selection import cross_val_score
scores = cross_val_score(clf, X, y, cv=5)  # Cambia el número de divisiones según tus necesidades
print("Cross-Validation Scores:", scores)

"""Observando estas métricas y realizando análisis más detallados, se podra evaluar el sesgo del modelo de árbol de decisión y determinar si está sesgado en una dirección particular o si es adecuado para el problema. Si se nota un sesgo significativo,  se puede ajustar el modelo, los hiperparámetros o los datos para abordar ese sesgo.

Predicciones y métricas del primer modelo:

Utilizas el modelo entrenado para hacer predicciones en el conjunto de prueba (X_test) y almacenas las predicciones en y_pred.
Calculas métricas de evaluación, como la precisión, la recuperación y la puntuación F1, utilizando las funciones de metrics.
"""

# Model Accuracy, how often is the classifier correct?
print("Accuracy:",metrics.accuracy_score(y_test, y_pred))
print("Precision:",metrics.precision_score(y_test, y_pred,labels=['0','1'],pos_label='1'))
print("Recall:",metrics.recall_score(y_test, y_pred,labels=['0','1'],pos_label='1'))
print("F1:",metrics.f1_score(y_test, y_pred,labels=['0','1'],pos_label='1'))

"""Visualización del primer árbol de decisión:

Exportas el árbol de decisión entrenado como un gráfico PNG utilizando export_graphviz y lo almacenas en un archivo llamado "diabetes.png". Luego, visualizas el gráfico utilizando pydotplus y Image
"""

from six import StringIO
from IPython.display import Image
from sklearn.tree import export_graphviz
import pydotplus
dot_data = StringIO()
export_graphviz(clf, out_file=dot_data,
                filled=True, rounded=True,
                special_characters=True, feature_names = feature_cols,class_names=['0','1'])
graph = pydotplus.graph_from_dot_data(dot_data.getvalue())
graph.write_png('diabetes_1.png')
Image(graph.create_png())

import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import learning_curve

def plot_learning_curve(clf, title, X, y, ylim=None, cv=None, n_jobs=None, train_sizes=np.linspace(.1, 1.0, 10)):
    plt.figure()
    plt.title(title)
    if ylim is not None:
        plt.ylim(*ylim)
    plt.xlabel("Número de ejemplos de entrenamiento")
    plt.ylabel("Puntuación")
    train_sizes, train_scores, test_scores = learning_curve(
        clf, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)
    train_scores_mean = np.mean(train_scores, axis=1)
    train_scores_std = np.std(train_scores, axis=1)
    test_scores_mean = np.mean(test_scores, axis=1)
    test_scores_std = np.std(test_scores, axis=1)
    plt.grid()

    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,
                     train_scores_mean + train_scores_std, alpha=0.1,
                     color="r")
    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,
                     test_scores_mean + test_scores_std, alpha=0.1, color="g")
    plt.plot(train_sizes, train_scores_mean, 'o-', color="r",
             label="Entrenamiento")
    plt.plot(train_sizes, test_scores_mean, 'o-', color="g",
             label="Validación cruzada")

    plt.legend(loc="best")
    return plt

# Usar tu modelo entrenado (clf), X_train y y_train
plot_learning_curve(clf, "Curva de Aprendizaje", X_train, y_train, cv=10)
plt.show()

"""#Creación y entrenamiento del segundo árbol de decisión (con entropía):

Creas un segundo objeto DecisionTreeClassifier llamado clf con el criterio de "entropía" y una profundidad máxima de 3.
Entrenas el nuevo modelo de árbol de decisión en el conjunto de entrenamiento (X_train, y_train) utilizando fit.

Predicciones y métricas del segundo modelo:

Utilizas el modelo entrenado para hacer predicciones en el conjunto de prueba (X_test) y almacenas las predicciones en y_pred.
Calculas métricas de evaluación, como la precisión, la recuperación y la puntuación F1, utilizando las funciones de metrics.
"""

# Create Decision Tree classifer object
clf = DecisionTreeClassifier(criterion="entropy", max_depth=3)

# Train Decision Tree Classifer
clf = clf.fit(X_train,y_train)

#Predict the response for test dataset
y_pred = clf.predict(X_test)

# Model Accuracy, how often is the classifier correct?
print("Accuracy:",metrics.accuracy_score(y_test, y_pred))
print("Precision:",metrics.precision_score(y_test, y_pred,labels=['0','1'],pos_label='1'))
print("Recall:",metrics.recall_score(y_test, y_pred,labels=['0','1'],pos_label='1'))
print("F1:",metrics.f1_score(y_test, y_pred,labels=['0','1'],pos_label='1'))

"""Visualización del segundo árbol de decisión (con entropía):

Exportas el segundo árbol de decisión entrenado como un gráfico PNG y lo visualizas de la misma manera que el primer árbol.
"""

from six import StringIO
from IPython.display import Image
from sklearn.tree import export_graphviz
import pydotplus
dot_data = StringIO()
export_graphviz(clf, out_file=dot_data,
                filled=True, rounded=True,
                special_characters=True, feature_names = feature_cols,class_names=['0','1'])
graph = pydotplus.graph_from_dot_data(dot_data.getvalue())
graph.write_png('diabetes.png')
Image(graph.create_png())

"""Este código carga el conjunto de datos de diabetes, entrena dos modelos de árbol de decisión con diferentes configuraciones (criterio de división y profundidad máxima) y evalúa su rendimiento. También visualiza ambos árboles de decisión para comprender cómo toman decisiones."""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import learning_curve

def plot_learning_curve(clf, title, X, y, ylim=None, cv=None, n_jobs=None, train_sizes=np.linspace(.1, 1.0, 5)):
    plt.figure()
    plt.title(title)
    if ylim is not None:
        plt.ylim(*ylim)
    plt.xlabel("Número de ejemplos de entrenamiento")
    plt.ylabel("Puntuación")
    train_sizes, train_scores, test_scores = learning_curve(
        clf, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)
    train_scores_mean = np.mean(train_scores, axis=1)
    train_scores_std = np.std(train_scores, axis=1)
    test_scores_mean = np.mean(test_scores, axis=1)
    test_scores_std = np.std(test_scores, axis=1)
    plt.grid()

    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,
                     train_scores_mean + train_scores_std, alpha=0.1,
                     color="r")
    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,
                     test_scores_mean + test_scores_std, alpha=0.1, color="g")
    plt.plot(train_sizes, train_scores_mean, 'o-', color="r",
             label="Entrenamiento")
    plt.plot(train_sizes, test_scores_mean, 'o-', color="g",
             label="Validación cruzada")

    plt.legend(loc="best")
    return plt

# Usar tu modelo entrenado (clf), X_train y y_train
plot_learning_curve(clf, "Curva de Aprendizaje", X_train, y_train, cv=5)
plt.show()

"""#Análisis de fit para saber como se encuentran nuestros dataset de train y test"""

from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Cargar y preprocesar tus datos en X e y

# Dividir el conjunto de datos en entrenamiento y prueba
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=50)

# Crear un modelo de árbol de decisión con diferentes profundidades
max_depths = [2, 4, 6, 8, 10]  # Diferentes profundidades máximas
for depth in max_depths:
    clf = DecisionTreeClassifier(max_depth=depth)
    clf.fit(X_train, y_train)

    # Predecir las etiquetas en el conjunto de entrenamiento y prueba
    y_train_pred = clf.predict(X_train)
    y_test_pred = clf.predict(X_test)

    # Calcular la precisión en el conjunto de entrenamiento y prueba
    train_accuracy = accuracy_score(y_train, y_train_pred)
    test_accuracy = accuracy_score(y_test, y_test_pred)

    print(f"Max Depth: {depth}")
    print(f"Training Accuracy: {train_accuracy:.2f}")
    print(f"Testing Accuracy: {test_accuracy:.2f}")

    # Evaluar si hay underfitting, buen ajuste u overfitting
    if train_accuracy > test_accuracy:
        print("El modelo podría estar overfitting.")
    elif train_accuracy < test_accuracy:
        print("El modelo podría estar underfitting.")
    else:
        print("El modelo tiene un buen ajuste.")